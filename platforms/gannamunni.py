import asyncio
import aiohttp
import json
import re
from typing import List, Dict, Optional
from dataclasses import dataclass
from datetime import datetime, date
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ (ì§ì ‘ ì‹¤í–‰ ì‹œ)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.logger import LoggedClass

@dataclass
class Writer:
    id: int
    doctor_id: Optional[int]
    profile: str
    nickname: str
    level: int
    engagement_type: Optional[str]

@dataclass
class Photo:
    url: str

@dataclass
class Comment:
    id: int
    status: int
    writer: Writer
    create_time: str
    reply_comment_id: Optional[int]
    is_admin: bool
    edited: bool
    contents: str
    has_thumb_up: bool
    thumb_up_count: int
    comment_count: int
    callee_nickname: Optional[str]
    lang: str
    translate_result: Optional[str]
    show_original_content: bool
    replies: List['Comment']

@dataclass
class ReviewAuthor:
    id: int
    level: int
    nickName: str
    profileImage: str

@dataclass
class ReviewTreatment:
    id: int
    name: str

@dataclass
class ReviewHospital:
    id: int
    name: str
    districtName: str
    country: str

@dataclass
class ReviewServiceOffer:
    id: int
    operationType: str

@dataclass
class ReviewCost:
    currency: str
    amount: int

@dataclass
class ReviewProgressPhoto:
    url: str
    progressDate: str

@dataclass
class ReviewAmplitudeTreatmentInfo:
    treatmentIdList: List[int]
    treatmentLabelList: List[str]
    treatmentCategoryTagIdList: List[int]
    treatmentCategoryTagLabelList: List[str]
    treatmentGroupTagIdList: List[int]
    treatmentGroupTagLabelList: List[str]
    concernTagIdList: List[int]
    concernTagLabelList: List[str]
    concernBodyPartTagIdList: List[int]
    concernBodyPartTagLabelList: List[str]

@dataclass
class Review:
    id: int
    author: ReviewAuthor
    treatments: List[ReviewTreatment]
    hospital: ReviewHospital
    serviceOffer: Optional[ReviewServiceOffer]
    totalRating: int
    totalCost: Optional[ReviewCost]
    description: str
    descriptionLanguage: str
    beforePhotos: List[str]
    afterPhotos: List[str]
    postedAtUtc: str
    editedLastAtUtc: str
    procedureProofApproved: bool
    amplitudeTreatmentInfo: ReviewAmplitudeTreatmentInfo
    highlights: List[str]
    progressReviewPhotos: List[ReviewProgressPhoto]
    treatmentReceivedAtUtc: str
    lastProgressDate: Optional[str]
    translation: Optional[str]
    costChangedReasons: Optional[Dict]
    isProgressOnGoing: Optional[bool]

@dataclass
class Article:
    id: int
    category_id: int
    category_name: str
    writer: Writer
    writer_doctor_id: Optional[int]
    has_thumb_up: bool
    comment_count: int
    thumb_up_count: int
    view_count: int
    create_time: str
    edited: bool
    title: str
    contents: str
    photos: List[Photo]
    lang: str
    has_doctor_comment: bool
    translate_result: Optional[str]
    is_admin: bool = False
    has_subscribed: bool = False
    comments: List[Comment] = None

class GangnamUnniAPI(LoggedClass):
    def __init__(self, token: str = "456c327614a94565b61f40f6683cda6c"):
        super().__init__("GangnamUnniAPI")
        self.base_url = "https://www.gangnamunni.com"
        self.token = token
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br, zstd",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "same-origin",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
        }
    
    async def get_article_list(self, category: str = "hospital_question", page: int = 1) -> List[Article]:
        """
        ê°•ë‚¨ì–¸ë‹ˆ ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            category: ì¹´í…Œê³ ë¦¬ (ê¸°ë³¸ê°’: "hospital_question" - ë³‘ì›ì§ˆë¬¸)
            page: í˜ì´ì§€ ë²ˆí˜¸ (ê¸°ë³¸ê°’: 1)
        
        Returns:
            List[Article]: ê²Œì‹œê¸€ ëª©ë¡
        """
        try:
            # í† í°ì„ í¬í•¨í•œ í—¤ë” ìƒì„±
            api_headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
                "Accept": "application/json, text/plain, */*",
                "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br, zstd",
                "Referer": "https://www.gangnamunni.com/",
                "Origin": "https://www.gangnamunni.com",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "empty",
                "Sec-Fetch-Mode": "cors",
                "Sec-Fetch-Site": "same-origin",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache",
                "Cookie": f"token={self.token}"
            }
            
            # ìƒˆë¡œìš´ solar API ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
            url = f"{self.base_url}/api/solar/search/document"
            
            # í˜ì´ì§€ ê³„ì‚° (APIëŠ” start íŒŒë¼ë¯¸í„° ì‚¬ìš©, ê³ ì • 20ê°œ)
            start = (page - 1) * 20
            
            # ì¹´í…Œê³ ë¦¬ ID ë§¤í•‘
            category_ids = {
                "hospital_question": 11,  # ë³‘ì›ì§ˆë¬¸
                "surgery_question": 2,    # ì‹œìˆ /ìˆ˜ìˆ ì§ˆë¬¸
                "free_chat": 1,           # ììœ ìˆ˜ë‹¤
                "review": 5,              # ë°œí’ˆí›„ê¸°
                "ask_doctor": 13,         # ì˜ì‚¬ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”
            }
            
            category_id = category_ids.get(category, 11)
            
            params = {
                "start": start,
                "length": 20,  # ê³ ì • 20ê°œ
                "sort": "createTime",
                "categoryIds": category_id
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, headers=api_headers) as response:
                    if response.status == 404:
                        raise Exception(f"404 Not Found: ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    elif response.status != 200:
                        raise Exception(f"HTTP {response.status}: {response.reason}")
                    
                    json_data = await response.json()
                    
                    # SUCCESS ì‘ë‹µ í™•ì¸
                    if json_data.get("reason") != "SUCCESS":
                        self.log_error(f"API ì‘ë‹µ ì˜¤ë¥˜: {json_data.get('reason')}")
                        return []
                    
                    # data ë°°ì—´ì—ì„œ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
                    articles_data = json_data.get("data", [])
                    
                    articles = []
                    for item in articles_data:
                        article = self._parse_article_from_solar_api(item)
                        articles.append(article)
                    
                    return articles
                    
        except Exception as e:
            self.log_error(f"ê²Œì‹œê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            # API ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return []
    
    async def get_articles_by_date(self, target_date: str, category: str = "hospital_question") -> List[Article]:
        """
        íŠ¹ì • ë‚ ì§œì˜ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Args:
            target_date: ìˆ˜ì§‘í•  ë‚ ì§œ (YYYY-MM-DD í˜•ì‹, ì˜ˆ: "2024-01-15")
            category: ì¹´í…Œê³ ë¦¬ (ê¸°ë³¸ê°’: "hospital_question" - ë³‘ì›ì§ˆë¬¸)
        
        Returns:
            List[Article]: í•´ë‹¹ ë‚ ì§œì˜ ê²Œì‹œê¸€ ëª©ë¡
        """
        try:
            # ë‚ ì§œ í˜•ì‹ ê²€ì¦
            target_date_obj = datetime.strptime(target_date, "%Y-%m-%d").date()
        except ValueError:
            self.log_error(f"ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ì…ë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì˜ˆ: 2024-01-15)")
            return []
        
        all_articles = []
        page = 1
        max_pages = 100  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì œí•œ
        consecutive_empty_pages = 0  # ì—°ì†ìœ¼ë¡œ ë¹ˆ í˜ì´ì§€ê°€ ë‚˜ì˜¨ íšŸìˆ˜
        max_consecutive_empty = 3  # ìµœëŒ€ ì—°ì† ë¹ˆ í˜ì´ì§€ ìˆ˜
        found_target_date = False  # ëª©í‘œ ë‚ ì§œ ê²Œì‹œê¸€ì„ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
        consecutive_404_errors = 0  # ì—°ì† 404 ì—ëŸ¬ íšŸìˆ˜
        max_404_errors = 5  # ìµœëŒ€ ì—°ì† 404 ì—ëŸ¬ í—ˆìš© íšŸìˆ˜
        
        while page <= max_pages and consecutive_empty_pages < max_consecutive_empty:
            try:
                # í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ê°€ì ¸ì˜¤ê¸°
                page_articles = await self.get_article_list(category=category, page=page)
                
                if not page_articles:
                    consecutive_empty_pages += 1
                    page += 1
                    await asyncio.sleep(1)
                    continue
                
                
                # ë‚ ì§œë³„ í•„í„°ë§
                target_date_articles = []
                older_articles_found = False
                
                for article in page_articles:
                    try:
                        article_date = self._parse_article_date(article.create_time)
                        
                        if article_date == target_date_obj:
                            target_date_articles.append(article)
                            found_target_date = True
                        elif article_date < target_date_obj:
                            # ë” ì˜¤ë˜ëœ ê²Œì‹œê¸€ì´ ë‚˜ì˜¤ë©´ ìˆ˜ì§‘ ì¤‘ë‹¨
                            older_articles_found = True
                            break
                        elif article_date > target_date_obj:
                            # ë” ìµœì‹  ê²Œì‹œê¸€ì´ ë‚˜ì˜¤ë©´ ê³„ì† ì§„í–‰
                            continue
                            
                    except (ValueError, IndexError) as e:
                        continue
                
                # í•´ë‹¹ ë‚ ì§œì˜ ê²Œì‹œê¸€ ì¶”ê°€
                if target_date_articles:
                    all_articles.extend(target_date_articles)
                
                # ë” ì˜¤ë˜ëœ ê²Œì‹œê¸€ì´ ë°œê²¬ë˜ë©´ ìˆ˜ì§‘ ì¤‘ë‹¨
                if older_articles_found:
                    break
                
                # í˜ì´ì§€ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(1)
                
                page += 1
                
            except Exception as e:
                error_msg = str(e)
                if "404" in error_msg or "Not Found" in error_msg:
                    consecutive_404_errors += 1
                    self.log_error(f"âŒ 404 ì—ëŸ¬ ë°œìƒ (ì—°ì† {consecutive_404_errors}íšŒ): {e}")
                    
                    if consecutive_404_errors >= max_404_errors:
                        self.log_error(f"ğŸš« ì—°ì† 404 ì—ëŸ¬ {max_404_errors}íšŒ ë°œìƒ. 20ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                        await asyncio.sleep(20 * 60)  # 20ë¶„ ëŒ€ê¸°
                        consecutive_404_errors = 0  # ì¹´ìš´í„° ë¦¬ì…‹
                    else:
                        await asyncio.sleep(5)  # 5ì´ˆ ëŒ€ê¸°
                else:
                    self.log_error(f"í˜ì´ì§€ {page} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    consecutive_empty_pages += 1
                
                page += 1
                await asyncio.sleep(2)
        
        return all_articles
    
    def _parse_article_date(self, time_str: str) -> date:
        """
        ê²Œì‹œê¸€ì˜ ì‹œê°„ ë¬¸ìì—´ì„ ë‚ ì§œë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            time_str: ì‹œê°„ ë¬¸ìì—´ (YYYY-MM-DD HH:MM:SS í˜•ì‹)
        
        Returns:
            date: íŒŒì‹±ëœ ë‚ ì§œ
        """
        try:
            # "YYYY-MM-DD HH:MM:SS" í˜•ì‹ì—ì„œ ë‚ ì§œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            date_str = time_str.split(' ')[0]
            result = datetime.strptime(date_str, "%Y-%m-%d").date()
            return result
        except (ValueError, IndexError) as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜¤ëŠ˜ ë‚ ì§œ ë°˜í™˜
            return datetime.now().date()
    
    async def get_comments(self, article_id: int) -> List[Comment]:
        """
        íŠ¹ì • ê²Œì‹œê¸€ì˜ ëŒ“ê¸€ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            article_id: ê²Œì‹œê¸€ ID
        
        Returns:
            List[Comment]: ëŒ“ê¸€ ëª©ë¡
        """
        self.log_info(f"        ğŸ” ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘: ê²Œì‹œê¸€ ID {article_id}")
        
        # ê²Œì‹œê¸€ë³„ 5ì´ˆ ë”œë ˆì´ (ê³¼ë¶€í•˜ ë°©ì§€)
        await asyncio.sleep(5)
        
        try:
            async with aiohttp.ClientSession(headers=self.headers) as session:
                # ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ URL
                url = f"{self.base_url}/community/{article_id}"
                self.log_info(f"        ğŸ“¡ í˜ì´ì§€ URL: {url}")
                
                async with session.get(url) as response:
                    self.log_info(f"        ğŸ“Š HTTP ìƒíƒœ: {response.status}")
                    
                    if response.status == 404:
                        error_msg = f"404 Not Found: ê²Œì‹œê¸€ ID {article_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                        self.log_error(f"        âŒ HTTP ì˜¤ë¥˜: {error_msg}")
                        raise Exception(error_msg)
                    elif response.status != 200:
                        error_msg = f"HTTP {response.status}: {response.reason}"
                        self.log_error(f"        âŒ HTTP ì˜¤ë¥˜: {error_msg}")
                        raise Exception(error_msg)
                    
                    html_content = await response.text()
                    self.log_info(f"        ğŸ“„ HTML í¬ê¸°: {len(html_content)} bytes")
                    
                    # __NEXT_DATA__ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ
                    import re
                    import json
                    
                    # __NEXT_DATA__ ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ì°¾ê¸°
                    next_data_pattern = r'<script id="__NEXT_DATA__" type="application/json">(.*?)</script>'
                    match = re.search(next_data_pattern, html_content, re.DOTALL)
                    
                    if not match:
                        self.log_error(f"        âŒ __NEXT_DATA__ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        return []
                    
                    try:
                        next_data = json.loads(match.group(1))
                        self.log_info(f"        âœ… __NEXT_DATA__ íŒŒì‹± ì„±ê³µ")
                        
                        # ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ
                        comments_data = next_data.get("props", {}).get("pageProps", {}).get("communityDocumentComments", [])
                        self.log_info(f"        ğŸ“‹ ì›ë³¸ ëŒ“ê¸€ ë°ì´í„°: {len(comments_data)}ê°œ")
                        
                        comments = []
                        for i, comment_data in enumerate(comments_data):
                            try:
                                comment = self._parse_comment_from_ssr(comment_data)
                                comments.append(comment)
                                self.log_info(f"        âœ… ëŒ“ê¸€ {i+1} íŒŒì‹± ì„±ê³µ: ID {comment.id}, ì‘ì„±ì {comment.writer.nickname}")
                            except Exception as parse_error:
                                self.log_warning(f"        âš ï¸  ëŒ“ê¸€ {i+1} íŒŒì‹± ì‹¤íŒ¨: {parse_error}")
                        
                        self.log_info(f"        ğŸ‰ ì´ {len(comments)}ê°œ ëŒ“ê¸€ íŒŒì‹± ì™„ë£Œ")
                        return comments
                        
                    except json.JSONDecodeError as e:
                        self.log_error(f"        âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        return []
                    
        except Exception as e:
            self.log_error(f"        âŒ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.log_error(f"        ğŸ” ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (pass ì²˜ë¦¬)
            return []
    
    def _parse_article_from_solar_api(self, data: Dict) -> Article:
        """
        ìƒˆë¡œìš´ solar API ì‘ë‹µì˜ ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ Article ê°ì²´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            data: solar API ì‘ë‹µì˜ ê²Œì‹œê¸€ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        
        Returns:
            Article: íŒŒì‹±ëœ ê²Œì‹œê¸€ ê°ì²´
        """
        # ì‘ì„±ì ì •ë³´ íŒŒì‹± (solar API í˜•ì‹)
        writer = Writer(
            id=data.get("writerId", 0),
            doctor_id=data.get("writerDoctorId"),
            profile=data.get("writerProfile", ""),
            nickname=data.get("writerNickName", ""),
            level=data.get("writerLevel", 1),
            engagement_type=data.get("writerEngagementType")
        )
        
        # ì‚¬ì§„ ì •ë³´ íŒŒì‹± (solar APIì—ì„œëŠ” ë¬¸ìì—´ ë°°ì—´)
        photos = []
        for photo_url in data.get("photos", []):
            photo = Photo(url=photo_url)
            photos.append(photo)
        
        # createTimeì„ íƒ€ì„ìŠ¤íƒ¬í”„ì—ì„œ ë¬¸ìì—´ë¡œ ë³€í™˜
        create_time_timestamp = data.get("createTime", 0)
        if create_time_timestamp:
            create_time_str = self._timestamp_to_readable_time(create_time_timestamp)
        else:
            create_time_str = ""
        
        # ê²Œì‹œê¸€ ê°ì²´ ìƒì„±
        article = Article(
            id=data.get("id", 0),
            category_id=data.get("categoryId", 0),
            category_name=data.get("categoryName", ""),
            writer=writer,
            writer_doctor_id=data.get("writerDoctorId"),
            has_thumb_up=data.get("hasThumbUp", False),
            comment_count=data.get("commentCount", 0),
            thumb_up_count=data.get("thumbUpCount", 0),
            view_count=data.get("viewCount", 0),
            create_time=create_time_str,
            edited=data.get("edited", False),
            title="",  # solar APIì—ëŠ” title í•„ë“œê°€ ì—†ìŒ
            contents=data.get("contents", ""),
            photos=photos,
            lang=data.get("lang", "ko"),
            has_doctor_comment=data.get("hasDoctorComment", False),
            translate_result=data.get("translateResult")
        )
        
        return article

    def _parse_article_from_api(self, data: Dict) -> Article:
        """
        API ì‘ë‹µì˜ ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ Article ê°ì²´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            data: API ì‘ë‹µì˜ ê²Œì‹œê¸€ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        
        Returns:
            Article: íŒŒì‹±ëœ ê²Œì‹œê¸€ ê°ì²´
        """
        # ì‘ì„±ì ì •ë³´ íŒŒì‹± (API í˜•ì‹)
        writer = Writer(
            id=data.get("writerId", 0),
            doctor_id=data.get("writerDoctorId"),
            profile=data.get("writerProfile", ""),
            nickname=data.get("writerNickName", ""),
            level=data.get("writerLevel", 1),
            engagement_type=data.get("writerEngagementType")
        )
        
        # ì‚¬ì§„ ì •ë³´ íŒŒì‹± (APIì—ì„œëŠ” ë¬¸ìì—´ ë°°ì—´)
        photos = []
        for photo_url in data.get("photos", []):
            photo = Photo(url=photo_url)
            photos.append(photo)
        
        # createTimeì„ íƒ€ì„ìŠ¤íƒ¬í”„ì—ì„œ ë¬¸ìì—´ë¡œ ë³€í™˜
        create_time_timestamp = data.get("createTime", 0)
        if create_time_timestamp:
            create_time_str = self._timestamp_to_readable_time(create_time_timestamp)
        else:
            create_time_str = ""
        
        # ê²Œì‹œê¸€ ê°ì²´ ìƒì„±
        article = Article(
            id=data.get("id", 0),
            category_id=data.get("categoryId", 0),
            category_name=data.get("categoryName", ""),
            writer=writer,
            writer_doctor_id=data.get("writerDoctorId"),
            has_thumb_up=data.get("hasThumbUp", False),
            comment_count=data.get("commentCount", 0),
            thumb_up_count=data.get("thumbUpCount", 0),
            view_count=data.get("viewCount", 0),
            create_time=create_time_str,
            edited=data.get("edited", False),
            title="",  # APIì—ëŠ” title í•„ë“œê°€ ì—†ìŒ
            contents=data.get("contents", ""),
            photos=photos,
            lang=data.get("lang", "ko"),
            has_doctor_comment=data.get("hasDoctorComment", False),
            translate_result=data.get("translateResult")
        )
        
        return article
    
    def _timestamp_to_readable_time(self, timestamp: int) -> str:
        """
        íƒ€ì„ìŠ¤íƒ¬í”„(ë°€ë¦¬ì´ˆ)ë¥¼ ì •í™•í•œ ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            timestamp: ë°€ë¦¬ì´ˆ íƒ€ì„ìŠ¤íƒ¬í”„
        
        Returns:
            str: ì •í™•í•œ ì‹œê°„ í˜•ì‹ (YYYY-MM-DD HH:MM:SS)
        """
        from datetime import datetime
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
        dt = datetime.fromtimestamp(timestamp / 1000)
        
        # ì •í™•í•œ ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    
    def _parse_article(self, data: Dict) -> Article:
        """
        ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ Article ê°ì²´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            data: ê²Œì‹œê¸€ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        
        Returns:
            Article: íŒŒì‹±ëœ ê²Œì‹œê¸€ ê°ì²´
        """
        # ì‘ì„±ì ì •ë³´ íŒŒì‹±
        writer_data = data.get("writer", {})
        writer = Writer(
            id=writer_data.get("id", 0),
            doctor_id=writer_data.get("doctorId"),
            profile=writer_data.get("profile", ""),
            nickname=writer_data.get("nickname", ""),
            level=writer_data.get("level", 1),
            engagement_type=writer_data.get("engagementType")
        )
        
        # ì‚¬ì§„ ì •ë³´ íŒŒì‹±
        photos = []
        for photo_data in data.get("photos", []):
            photo = Photo(url=photo_data.get("url", ""))
            photos.append(photo)
        
        # ê²Œì‹œê¸€ ê°ì²´ ìƒì„±
        article = Article(
            id=data.get("id", 0),
            category_id=data.get("categoryId", 0),
            category_name=data.get("categoryName", ""),
            writer=writer,
            writer_doctor_id=data.get("writerDoctorId"),
            has_thumb_up=data.get("hasThumbUp", False),
            comment_count=data.get("commentCount", 0),
            thumb_up_count=data.get("thumbUpCount", 0),
            view_count=data.get("viewCount", 0),
            create_time=data.get("createTime", ""),
            edited=data.get("edited", False),
            title=data.get("title", ""),
            contents=data.get("contents", ""),
            photos=photos,
            lang=data.get("lang", "ko"),
            has_doctor_comment=data.get("hasDoctorComment", False),
            translate_result=data.get("translateResult")
        )
        
        return article
    
    def _parse_comment_from_ssr(self, data: Dict) -> Comment:
        """
        SSR ë°ì´í„°ì˜ ëŒ“ê¸€ ë°ì´í„°ë¥¼ Comment ê°ì²´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            data: SSR ëŒ“ê¸€ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        
        Returns:
            Comment: íŒŒì‹±ëœ ëŒ“ê¸€ ê°ì²´
        """
        try:
            # ëŒ“ê¸€ ì‘ì„±ì ì •ë³´ íŒŒì‹± (SSR í˜•ì‹)
            writer_data = data.get("writer", {})
            writer = Writer(
                id=writer_data.get("id", 0),
                doctor_id=writer_data.get("doctorId"),
                profile=writer_data.get("profile", ""),
                nickname=writer_data.get("nickname", ""),
                level=writer_data.get("level", 1),
                engagement_type=writer_data.get("engagementType")
            )
            
            # createTimeì€ ì´ë¯¸ ë¬¸ìì—´ í˜•íƒœ
            create_time_str = data.get("createTime", "")
            
            # ëŒ€ëŒ“ê¸€ íŒŒì‹±
            replies = []
            replies_data = data.get("replies", [])
            if replies_data:
                self.log_info(f"          ğŸ”„ ëŒ€ëŒ“ê¸€ {len(replies_data)}ê°œ íŒŒì‹± ì¤‘...")
                for reply_data in replies_data:
                    reply = self._parse_comment_from_ssr(reply_data)
                    replies.append(reply)
            
            # ëŒ“ê¸€ ê°ì²´ ìƒì„±
            comment = Comment(
                id=data.get("id", 0),
                status=data.get("status", 1),
                writer=writer,
                create_time=create_time_str,
                reply_comment_id=data.get("replyCommentId"),
                is_admin=data.get("isAdmin", False),
                edited=data.get("edited", False),
                contents=data.get("contents", ""),
                has_thumb_up=data.get("hasThumbUp", False),
                thumb_up_count=data.get("thumbUpCount", 0),
                comment_count=data.get("commentCount", 0),
                callee_nickname=data.get("calleeNickName"),
                lang=data.get("lang", "ko"),
                translate_result=data.get("translateResult"),
                show_original_content=data.get("showOriginalContent", True),
                replies=replies
            )
            
            return comment
            
        except Exception as e:
            self.log_error(f"          âŒ ëŒ“ê¸€ íŒŒì‹± ì‹¤íŒ¨: {e}")
            self.log_error(f"          ğŸ“‹ ì›ë³¸ ë°ì´í„°: {data}")
            raise e
    
    def _parse_comment_from_api(self, data: Dict) -> Comment:
        """
        API ì‘ë‹µì˜ ëŒ“ê¸€ ë°ì´í„°ë¥¼ Comment ê°ì²´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            data: API ì‘ë‹µì˜ ëŒ“ê¸€ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        
        Returns:
            Comment: íŒŒì‹±ëœ ëŒ“ê¸€ ê°ì²´
        """
        try:
            # ëŒ“ê¸€ ì‘ì„±ì ì •ë³´ íŒŒì‹± (API í˜•ì‹)
            writer = Writer(
                id=data.get("writerId", 0),
                doctor_id=data.get("writerDoctorId"),
                profile=data.get("writerProfile", ""),
                nickname=data.get("writerNickName", ""),
                level=data.get("writerLevel", 1),
                engagement_type=data.get("writerEngagementType")
            )
            
            # createTimeì„ íƒ€ì„ìŠ¤íƒ¬í”„ì—ì„œ ë¬¸ìì—´ë¡œ ë³€í™˜
            create_time_timestamp = data.get("createTime", 0)
            if create_time_timestamp:
                create_time_str = self._timestamp_to_readable_time(create_time_timestamp)
            else:
                create_time_str = ""
            
            # ëŒ€ëŒ“ê¸€ íŒŒì‹±
            replies = []
            replies_data = data.get("replies", [])
            if replies_data:
                self.log_info(f"          ğŸ”„ ëŒ€ëŒ“ê¸€ {len(replies_data)}ê°œ íŒŒì‹± ì¤‘...")
                for reply_data in replies_data:
                    reply = self._parse_comment_from_api(reply_data)
                    replies.append(reply)
            
            # ëŒ“ê¸€ ê°ì²´ ìƒì„±
            comment = Comment(
                id=data.get("id", 0),
                status=data.get("status", 1),
                writer=writer,
                create_time=create_time_str,
                reply_comment_id=data.get("replyCommentId"),
                is_admin=data.get("isAdmin", False),
                edited=data.get("edited", False),
                contents=data.get("contents", ""),
                has_thumb_up=data.get("hasThumbUp", False),
                thumb_up_count=data.get("thumbUpCount", 0),
                comment_count=data.get("commentCount", 0),
                callee_nickname=data.get("calleeNickName"),
                lang=data.get("lang", "ko"),
                translate_result=data.get("translateResult"),
                show_original_content=data.get("showOriginalContent", True),
                replies=replies
            )
            
            return comment
            
        except Exception as e:
            self.log_error(f"          âŒ ëŒ“ê¸€ íŒŒì‹± ì‹¤íŒ¨: {e}")
            self.log_error(f"          ğŸ“‹ ì›ë³¸ ë°ì´í„°: {data}")
            raise e
    
    async def search_articles(self, keyword: str, category: str = "hospital_question") -> List[Article]:
        """
        í‚¤ì›Œë“œë¡œ ê²Œì‹œê¸€ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            category: ì¹´í…Œê³ ë¦¬ (ê¸°ë³¸ê°’: "hospital_question")
        
        Returns:
            List[Article]: ê²€ìƒ‰ ê²°ê³¼ ê²Œì‹œê¸€ ëª©ë¡
        """
        try:
            # ì „ì²´ ê²Œì‹œê¸€ ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ í‚¤ì›Œë“œ í•„í„°ë§
            articles = await self.get_article_list(category=category, page=1)
            
            filtered_articles = []
            for article in articles:
                if (keyword.lower() in article.title.lower() or 
                    keyword.lower() in article.contents.lower() or
                    keyword.lower() in article.writer.nickname.lower()):
                    filtered_articles.append(article)
            
            return filtered_articles
            
        except Exception as e:
            self.log_error(f"ê²Œì‹œê¸€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    async def get_reviews(self, page_index: int = 0, page_size: int = 20, sort: str = "RECENT_POSTED_AT", keyword: str = "ì„±í˜•") -> List[Review]:
        """
        ê°•ë‚¨ì–¸ë‹ˆ ë¦¬ë·° ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            page_index: í˜ì´ì§€ ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: 0)
            page_size: í˜ì´ì§€ë‹¹ ë¦¬ë·° ìˆ˜ (ê¸°ë³¸ê°’: 20)
            sort: ì •ë ¬ ë°©ì‹ (ê¸°ë³¸ê°’: "RECENT_POSTED_AT")
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ (ê¸°ë³¸ê°’: "ì„±í˜•" - ëª¨ë“  ë¦¬ë·° ì¡°íšŒìš©)
        
        Returns:
            List[Review]: ë¦¬ë·° ëª©ë¡
        """
        try:
            # ë¦¬ë·° API ì—”ë“œí¬ì¸íŠ¸
            url = "https://env.gnsister.com/display/search-view/reviews"
            
            # ìš”ì²­ í—¤ë”
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
                "Accept": "application/json, text/plain, */*",
                "Authorization": self.token
            }
            
            # ìš”ì²­ ë°”ë””
            payload = {
                "filters": {
                    "gender": "",
                    "hasPhotos": False,
                    "procedureProofApproved": False
                },
                "keyword": "ê³ ë¯¼",
                "pagination": {
                    "pageIndex": page_index,
                    "pageSize": page_size,
                    "sort": sort
                }
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload, headers=headers) as response:
                    if response.status != 200:
                        raise Exception(f"HTTP {response.status}: {response.reason}")
                    
                    json_data = await response.json()
                    
                    # contents ë°°ì—´ì—ì„œ ë¦¬ë·° ëª©ë¡ ì¶”ì¶œ
                    reviews_data = json_data.get("contents", [])
                    
                    reviews = []
                    for item in reviews_data:
                        review = self._parse_review_from_api(item)
                        reviews.append(review)
                    
                    return reviews
                    
        except Exception as e:
            self.log_error(f"ë¦¬ë·° ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []

    async def get_reviews_by_date(self, target_date: str, max_pages: int = 50) -> List[Review]:
        """
        íŠ¹ì • ë‚ ì§œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Args:
            target_date: ìˆ˜ì§‘í•  ë‚ ì§œ (YYYY-MM-DD í˜•ì‹, ì˜ˆ: "2024-01-15")
            max_pages: ìµœëŒ€ ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 50)
        
        Returns:
            List[Review]: í•´ë‹¹ ë‚ ì§œì˜ ë¦¬ë·° ëª©ë¡
        """
        try:
            # ë‚ ì§œ í˜•ì‹ ê²€ì¦
            target_date_obj = datetime.strptime(target_date, "%Y-%m-%d").date()
        except ValueError:
            self.log_error(f"ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ì…ë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì˜ˆ: 2024-01-15)")
            return []
        
        all_reviews = []
        page_index = 0
        consecutive_empty_pages = 0
        max_consecutive_empty = 3
        found_target_date = False
        consecutive_404_errors = 0
        max_404_errors = 5
        
        while page_index < max_pages and consecutive_empty_pages < max_consecutive_empty:
            try:
                # í˜„ì¬ í˜ì´ì§€ì˜ ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
                page_reviews = await self.get_reviews(page_index=page_index, page_size=20)
                
                self.log_info(f"ğŸ“„ í˜ì´ì§€ {page_index + 1}: {len(page_reviews)}ê°œ ë¦¬ë·° ì¡°íšŒ")
                
                if not page_reviews:
                    consecutive_empty_pages += 1
                    self.log_info(f"ğŸ“­ ë¹ˆ í˜ì´ì§€ {consecutive_empty_pages}íšŒ ì—°ì†")
                    page_index += 1
                    await asyncio.sleep(1)
                    continue
                else:
                    consecutive_empty_pages = 0  # ë¦¬ë·°ê°€ ìˆìœ¼ë©´ ì¹´ìš´í„° ë¦¬ì…‹
                
                # ë‚ ì§œë³„ í•„í„°ë§
                target_date_reviews = []
                older_reviews_found = False
                
                for review in page_reviews:
                    try:
                        # postedAtUtcë¥¼ ë‚ ì§œë¡œ ë³€í™˜
                        review_date = self._parse_review_date(review.postedAtUtc)
                        
                        if review_date == target_date_obj:
                            target_date_reviews.append(review)
                            found_target_date = True
                        elif review_date < target_date_obj:
                            # ë” ì˜¤ë˜ëœ ë¦¬ë·°ê°€ ë‚˜ì˜¤ë©´ ìˆ˜ì§‘ ì¤‘ë‹¨
                            older_reviews_found = True
                            break
                        elif review_date > target_date_obj:
                            # ë” ìµœì‹  ë¦¬ë·°ê°€ ë‚˜ì˜¤ë©´ ê³„ì† ì§„í–‰
                            continue
                            
                    except (ValueError, IndexError) as e:
                        continue
                
                # í•´ë‹¹ ë‚ ì§œì˜ ë¦¬ë·° ì¶”ê°€
                if target_date_reviews:
                    all_reviews.extend(target_date_reviews)
                    self.log_info(f"âœ… í˜ì´ì§€ {page_index + 1}: {len(target_date_reviews)}ê°œ ë¦¬ë·° ì¶”ê°€ (ì´ {len(all_reviews)}ê°œ)")
                else:
                    self.log_info(f"ğŸ“… í˜ì´ì§€ {page_index + 1}: í•´ë‹¹ ë‚ ì§œ ë¦¬ë·° ì—†ìŒ")
                
                # ë” ì˜¤ë˜ëœ ë¦¬ë·°ê°€ ë°œê²¬ë˜ë©´ ìˆ˜ì§‘ ì¤‘ë‹¨
                if older_reviews_found:
                    self.log_info(f"ğŸ›‘ ë” ì˜¤ë˜ëœ ë¦¬ë·° ë°œê²¬ìœ¼ë¡œ ìˆ˜ì§‘ ì¤‘ë‹¨ (í˜ì´ì§€ {page_index + 1})")
                    break
                
                # í˜ì´ì§€ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(1)
                
                page_index += 1
                
            except Exception as e:
                error_msg = str(e)
                if "404" in error_msg or "Not Found" in error_msg:
                    consecutive_404_errors += 1
                    self.log_error(f"âŒ 404 ì—ëŸ¬ ë°œìƒ (ì—°ì† {consecutive_404_errors}íšŒ): {e}")
                    
                    if consecutive_404_errors >= max_404_errors:
                        self.log_error(f"ğŸš« ì—°ì† 404 ì—ëŸ¬ {max_404_errors}íšŒ ë°œìƒ. 20ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                        await asyncio.sleep(20 * 60)  # 20ë¶„ ëŒ€ê¸°
                        consecutive_404_errors = 0  # ì¹´ìš´í„° ë¦¬ì…‹
                    else:
                        await asyncio.sleep(5)  # 5ì´ˆ ëŒ€ê¸°
                else:
                    self.log_error(f"í˜ì´ì§€ {page_index} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                    consecutive_empty_pages += 1
                
                page_index += 1
                await asyncio.sleep(2)
        
        self.log_info(f"ğŸ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_reviews)}ê°œ (í˜ì´ì§€ {page_index}ê°œ ì²˜ë¦¬)")
        return all_reviews

    async def get_review_detail(self, review_id: int) -> Optional[dict]:
        """ë¦¬ë·° ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        url = "https://env.gnsister.com/review/query/page/user/procedure-journey/v1/review-detail/main"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Authorization": self.token
        }
        payload = {"id": review_id}
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload, headers=headers) as response:
                    if response.status != 200:
                        self.log_error(f"âŒ ë¦¬ë·° ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨ (ID: {review_id}): HTTP {response.status}")
                        return None
                    
                    json_data = await response.json()
                    return json_data
                    
        except Exception as e:
            self.log_error(f"âŒ ë¦¬ë·° ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ (ID: {review_id}): {e}")
            return None

    def _parse_review_date(self, utc_time_str: str) -> date:
        """
        ë¦¬ë·°ì˜ UTC ì‹œê°„ ë¬¸ìì—´ì„ ë‚ ì§œë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            utc_time_str: UTC ì‹œê°„ ë¬¸ìì—´ (YYYY-MM-DDTHH:MM:SSZ í˜•ì‹)
        
        Returns:
            date: íŒŒì‹±ëœ ë‚ ì§œ
        """
        try:
            # "YYYY-MM-DDTHH:MM:SSZ" í˜•ì‹ì—ì„œ ë‚ ì§œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            date_str = utc_time_str.split('T')[0]
            result = datetime.strptime(date_str, "%Y-%m-%d").date()
            return result
        except (ValueError, IndexError) as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜¤ëŠ˜ ë‚ ì§œ ë°˜í™˜
            return datetime.now().date()

    def _parse_review_from_api(self, data: Dict) -> Review:
        """
        ë¦¬ë·° API ì‘ë‹µì˜ ë°ì´í„°ë¥¼ Review ê°ì²´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            data: ë¦¬ë·° API ì‘ë‹µì˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        
        Returns:
            Review: íŒŒì‹±ëœ ë¦¬ë·° ê°ì²´
        """
        # ì‘ì„±ì ì •ë³´ íŒŒì‹±
        author_data = data.get("author", {})
        author = ReviewAuthor(
            id=author_data.get("id", 0),
            level=author_data.get("level", 1),
            nickName=author_data.get("nickName", ""),
            profileImage=author_data.get("profileImage", "")
        )
        
        # ì‹œìˆ  ì •ë³´ íŒŒì‹±
        treatments = []
        for treatment_data in data.get("treatments", []):
            treatment = ReviewTreatment(
                id=treatment_data.get("id", 0),
                name=treatment_data.get("name", "")
            )
            treatments.append(treatment)
        
        # ë³‘ì› ì •ë³´ íŒŒì‹±
        hospital_data = data.get("hospital", {})
        hospital = ReviewHospital(
            id=hospital_data.get("id", 0),
            name=hospital_data.get("name", ""),
            districtName=hospital_data.get("districtName", ""),
            country=hospital_data.get("country", "")
        )
        
        # ì„œë¹„ìŠ¤ ì˜¤í¼ ì •ë³´ íŒŒì‹±
        service_offer = None
        service_offer_data = data.get("serviceOffer")
        if service_offer_data:
            service_offer = ReviewServiceOffer(
                id=service_offer_data.get("id", 0),
                operationType=service_offer_data.get("operationType", "")
            )
        
        # ë¹„ìš© ì •ë³´ íŒŒì‹±
        total_cost = None
        cost_data = data.get("totalCost")
        if cost_data:
            total_cost = ReviewCost(
                currency=cost_data.get("currency", ""),
                amount=cost_data.get("amount", 0)
            )
        
        # ì§„í–‰ ì‚¬ì§„ íŒŒì‹±
        progress_photos = []
        for photo_data in data.get("progressReviewPhotos", []):
            progress_photo = ReviewProgressPhoto(
                url=photo_data.get("url", ""),
                progressDate=photo_data.get("progressDate", "")
            )
            progress_photos.append(progress_photo)
        
        # ì‹œìˆ  ì •ë³´ íŒŒì‹±
        amplitude_data = data.get("amplitudeTreatmentInfo", {})
        amplitude_treatment_info = ReviewAmplitudeTreatmentInfo(
            treatmentIdList=amplitude_data.get("treatmentIdList", []),
            treatmentLabelList=amplitude_data.get("treatmentLabelList", []),
            treatmentCategoryTagIdList=amplitude_data.get("treatmentCategoryTagIdList", []),
            treatmentCategoryTagLabelList=amplitude_data.get("treatmentCategoryTagLabelList", []),
            treatmentGroupTagIdList=amplitude_data.get("treatmentGroupTagIdList", []),
            treatmentGroupTagLabelList=amplitude_data.get("treatmentGroupTagLabelList", []),
            concernTagIdList=amplitude_data.get("concernTagIdList", []),
            concernTagLabelList=amplitude_data.get("concernTagLabelList", []),
            concernBodyPartTagIdList=amplitude_data.get("concernBodyPartTagIdList", []),
            concernBodyPartTagLabelList=amplitude_data.get("concernBodyPartTagLabelList", [])
        )
        
        # ë¦¬ë·° ê°ì²´ ìƒì„±
        review = Review(
            id=data.get("id", 0),
            author=author,
            treatments=treatments,
            hospital=hospital,
            serviceOffer=service_offer,
            totalRating=data.get("totalRating", 0),
            totalCost=total_cost,
            description=data.get("description", ""),
            descriptionLanguage=data.get("descriptionLanguage", "ko"),
            beforePhotos=data.get("beforePhotos", []),
            afterPhotos=data.get("afterPhotos", []),
            postedAtUtc=data.get("postedAtUtc", ""),
            editedLastAtUtc=data.get("editedLastAtUtc", ""),
            procedureProofApproved=data.get("procedureProofApproved", False),
            amplitudeTreatmentInfo=amplitude_treatment_info,
            highlights=data.get("highlights", []),
            progressReviewPhotos=progress_photos,
            treatmentReceivedAtUtc=data.get("treatmentReceivedAtUtc", ""),
            lastProgressDate=data.get("lastProgressDate"),
            translation=data.get("translation"),
            costChangedReasons=data.get("costChangedReasons"),
            isProgressOnGoing=data.get("isProgressOnGoing")
        )
        
        return review

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_gannamunni_api():
    """ê°•ë‚¨ì–¸ë‹ˆ API í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    from utils.logger import get_logger
    logger = get_logger("GANNAMUNNI_TEST")
    
    logger.info("ğŸ§ª ê°•ë‚¨ì–¸ë‹ˆ API í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 50)
    
    api = GangnamUnniAPI()
    
    try:
        # ê²Œì‹œê¸€ ëª©ë¡ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“ ê²Œì‹œê¸€ ëª©ë¡ í…ŒìŠ¤íŠ¸")
        articles = await api.get_article_list(category="hospital_question", page=1)
        
        logger.info(f"\nğŸ“Š ê²Œì‹œê¸€ ëª©ë¡ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        logger.info(f"   ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {len(articles)}ê°œ")
        
        if articles:
            logger.info(f"\nğŸ“ ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´:")
            first_article = articles[0]
            logger.info(f"   ID: {first_article.id}")
            logger.info(f"   ì‘ì„±ì: {first_article.writer.nickname}")
            logger.info(f"   ì¹´í…Œê³ ë¦¬: {first_article.category_name}")
            logger.info(f"   ì¡°íšŒìˆ˜: {first_article.view_count}")
            logger.info(f"   ëŒ“ê¸€ ìˆ˜: {first_article.comment_count}")
            logger.info(f"   ì‘ì„±ì‹œê°„: {first_article.create_time}")
            logger.info(f"   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {first_article.contents[:100]}...")
        
        # ë‚ ì§œë³„ ê²Œì‹œê¸€ í…ŒìŠ¤íŠ¸
        logger.info(f"\nğŸ“… ë‚ ì§œë³„ ê²Œì‹œê¸€ í…ŒìŠ¤íŠ¸")
        from datetime import datetime
        today = datetime.now().strftime("%Y-%m-%d")
        logger.info(f"ğŸ“… ì˜¤ëŠ˜ ë‚ ì§œ({today}) ê²Œì‹œê¸€ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
        
        date_articles = await api.get_articles_by_date(today, category="hospital_question")
        
        logger.info(f"ğŸ“Š ë‚ ì§œë³„ ê²Œì‹œê¸€ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        logger.info(f"   ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {len(date_articles)}ê°œ")
        
        if date_articles:
            logger.info(f"\nğŸ“ ì²« ë²ˆì§¸ ë‚ ì§œë³„ ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´:")
            first_date_article = date_articles[0]
            logger.info(f"   ID: {first_date_article.id}")
            logger.info(f"   ì‘ì„±ì: {first_date_article.writer.nickname}")
            logger.info(f"   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {first_date_article.contents[:100]}...")
        
        # ëŒ“ê¸€ í…ŒìŠ¤íŠ¸ (ê²Œì‹œê¸€ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if articles and articles[0].comment_count > 0:
            logger.info(f"\nğŸ’¬ ëŒ“ê¸€ í…ŒìŠ¤íŠ¸")
            comments = await api.get_comments(articles[0].id)
            
            logger.info(f"ğŸ“Š ëŒ“ê¸€ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            logger.info(f"   ìˆ˜ì§‘ëœ ëŒ“ê¸€: {len(comments)}ê°œ")
            
            if comments:
                logger.info(f"\nğŸ“ ì²« ë²ˆì§¸ ëŒ“ê¸€ ìƒì„¸ ì •ë³´:")
                first_comment = comments[0]
                logger.info(f"   ID: {first_comment.id}")
                logger.info(f"   ì‘ì„±ì: {first_comment.writer.nickname}")
                logger.info(f"   ë‚´ìš©: {first_comment.contents}")
                logger.info(f"   ì‘ì„±ì‹œê°„: {first_comment.create_time}")
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    logger.info("=" * 50)
    logger.info("ğŸ§ª ê°•ë‚¨ì–¸ë‹ˆ API í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

async def test_get_reviews():
    from utils.logger import get_logger
    logger = get_logger("GANNAMUNNI_TEST")
    
    logger.info("ğŸ§ª ê°•ë‚¨ì–¸ë‹ˆ API í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 50)
    
    api = GangnamUnniAPI("ca06262d608b4ea3be4cc026454081cd")
    
    # get_reviews í•¨ìˆ˜ í˜¸ì¶œ í…ŒìŠ¤íŠ¸
    logger.info(f"\nğŸ§ª get_reviews í•¨ìˆ˜ í˜¸ì¶œ í…ŒìŠ¤íŠ¸")
    try:
        reviews = await api.get_reviews(page_index=100, page_size=20)
        logger.info(f"ğŸ“Š get_reviews ê²°ê³¼: {len(reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ë¨")
        if reviews:
            # print(reviews)
            # logger.info(f"ğŸ“ ì²« ë²ˆì§¸ ë¦¬ë·° ì •ë³´:")
            # first_review = reviews[0]
            pass
            
    except Exception as e:
        logger.error(f"âŒ get_reviews í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    asyncio.run(test_get_reviews())

