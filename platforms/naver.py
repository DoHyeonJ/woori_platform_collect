#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ì¹´í˜ API í´ë¼ì´ì–¸íŠ¸
"""
import aiohttp
import asyncio
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import html
import logging

from utils.logger import LoggedClass

@dataclass
class NaverCafeMenu:
    """ë„¤ì´ë²„ ì¹´í˜ ë©”ë‰´ ì •ë³´"""
    menu_id: int
    menu_name: str
    menu_type: str
    board_type: str
    sort: int

@dataclass
class NaverCafeArticle:
    """ë„¤ì´ë²„ ì¹´í˜ ê²Œì‹œê¸€ ì •ë³´"""
    article_id: str
    subject: str
    writer_nickname: str
    writer_id: str
    content: Optional[str] = None
    created_at: Optional[datetime] = None
    view_count: Optional[int] = None
    comment_count: Optional[int] = None
    like_count: Optional[int] = None

class NaverCafeAPI(LoggedClass):
    """ë„¤ì´ë²„ ì¹´í˜ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, naver_cookies: str = ""):
        super().__init__()
        self.naver_cookies = naver_cookies
        self.base_url = "https://apis.naver.com"
        
        # ê¸°ë³¸ í—¤ë” ì„¤ì •
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "application/json",
            "Referer": "https://cafe.naver.com/"
        }
        
        # ì¿ í‚¤ê°€ ìˆëŠ” ê²½ìš° í—¤ë”ì— ì¶”ê°€
        if naver_cookies and naver_cookies.strip():
            self.headers["Cookie"] = naver_cookies.strip()
            self.log_info(f"ë„¤ì´ë²„ ì¿ í‚¤ ì„¤ì •ë¨: {naver_cookies[:50]}...")
        else:
            self.log_warning("ë„¤ì´ë²„ ì¿ í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¼ë¶€ API í˜¸ì¶œì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì£¼ìš” ì¹´í˜ ID ëª©ë¡
        self.cafe_ids = {
            "ì—¬ìš°ì•¼": "10912875",
            "A+ì—¬ìš°ì•¼": "12285441", 
            "ì„±í˜•ìœ„í‚¤ë°±ê³¼": "11498714",
            "ì—¬ìƒë‚¨ì •": "13067396",
            "ì‹œí¬ë¨¼íŠ¸": "23451561",
            "ê°€ì•„ì‚¬": "15880379",
            "íŒŒìš°ë”ë£¸": "10050813"
        }
    
    async def get_board_list(self, cafe_id: str) -> List[NaverCafeMenu]:
        """ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ"""
        try:
            self.log_info(f"ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ ì‹œì‘ (ì¹´í˜ ID: {cafe_id})")
            
            # ìƒˆë¡œìš´ ë„¤ì´ë²„ API ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
            url = f"https://article.cafe.naver.com/gw/v3/cafes/{cafe_id}/menus"
            params = {
                "useCafeId": "true",
                "requestFrom": "A"
            }
            
            self.log_info(f"ìš”ì²­ URL: {url}")
            self.log_info(f"ìš”ì²­ íŒŒë¼ë¯¸í„°: {params}")
            self.log_info(f"ìš”ì²­ í—¤ë”: {self.headers}")
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, headers=self.headers) as response:
                    self.log_info(f"ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status}")
                    self.log_info(f"ì‘ë‹µ í—¤ë”: {dict(response.headers)}")
                    
                    if response.status == 200:
                        data = await response.json()
                        self.log_info(f"ì‘ë‹µ ë°ì´í„° í‚¤: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
                        
                        # ìƒˆë¡œìš´ API ì‘ë‹µ êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •
                        if 'result' in data and 'menus' in data['result']:
                            menus = data['result']['menus']
                            self.log_info(f"ë©”ë‰´ ê°œìˆ˜: {len(menus)}ê°œ")
                            
                            menu_list = []
                            for menu in menus:
                                # P(í”„ë¡œí•„), L(ë§í¬), F(í´ë”) ì œì™¸
                                if menu.get('menuType') not in ['P', 'L', 'F']:
                                    menu_list.append(NaverCafeMenu(
                                        menu_id=int(menu.get('id', 0)),
                                        menu_name=html.unescape(menu.get('name', '')),
                                        menu_type=menu.get('menuType', ''),
                                        board_type=menu.get('boardType', ''),
                                        sort=menu.get('sort', 0)
                                    ))
                            
                            # ì •ë ¬ ìˆœì„œëŒ€ë¡œ ì •ë ¬
                            menu_list.sort(key=lambda x: x.sort)
                            self.log_info(f"ê²Œì‹œíŒ {len(menu_list)}ê°œ ì¡°íšŒ ì™„ë£Œ")
                            return menu_list
                        else:
                            self.log_error("ê²Œì‹œíŒ ëª©ë¡ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
                            self.log_error(f"ì‘ë‹µ êµ¬ì¡°: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
                            if 'result' in data:
                                self.log_error(f"result êµ¬ì¡°: {list(data['result'].keys()) if isinstance(data['result'], dict) else 'Not a dict'}")
                            return []
                    else:
                        response_text = await response.text()
                        self.log_error(f"ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: HTTP {response.status}")
                        self.log_error(f"ì‘ë‹µ ë‚´ìš©: {response_text}")
                        return []
                        
        except Exception as e:
            self.log_error(f"ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            self.log_error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []
    
    async def get_article_list(self, cafe_id: str, menu_id: str = "", page: int = 1, per_page: int = 20) -> List[NaverCafeArticle]:
        """ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ"""
        try:
            self.log_info(f"ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ ì‹œì‘ (ì¹´í˜ ID: {cafe_id}, ë©”ë‰´ ID: {menu_id}, í˜ì´ì§€: {page})")
            
            url = f"{self.base_url}/cafe-web/cafe2/ArticleListV2dot1.json"
            params = {
                "search.clubid": cafe_id,
                "search.queryType": "lastArticle",
                "search.menuid": menu_id,
                "search.page": page,
                "search.perPage": per_page,
                "adUnit": "MW_CAFE_ARTICLE_LIST_RS"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, headers=self.headers) as response:
                    self.log_info(f"ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status}")
                    
                    if response.status == 200:
                        data = await response.json()
                        self.log_info(f"ì‘ë‹µ ë°ì´í„° í‚¤: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
                        
                        article_list = []
                        if 'message' in data and 'result' in data['message'] and 'articleList' in data['message']['result']:
                            articles = data['message']['result']['articleList']
                            # ê²Œì‹œê¸€ ID ìˆœìœ¼ë¡œ ì •ë ¬
                            articles.sort(key=lambda x: x['articleId'])
                            
                            for article in articles:
                                # writeDateë¥¼ Unix timestamp (ë°€ë¦¬ì´ˆ)ì—ì„œ datetimeìœ¼ë¡œ ë³€í™˜
                                created_at = self._convert_write_date(article.get('writeDateTimestamp'))
                                
                                article_list.append(NaverCafeArticle(
                                    article_id=article['articleId'],
                                    subject=article['subject'],
                                    writer_nickname=article['writerNickname'],
                                    writer_id=article.get('writerId', ''),
                                    created_at=created_at,
                                    view_count=article.get('readCount', 0),
                                    comment_count=article.get('commentCount', 0),
                                    like_count=article.get('likeCount', 0)
                                ))
                            
                            self.log_info(f"ê²Œì‹œê¸€ {len(article_list)}ê°œ ì¡°íšŒ ì™„ë£Œ")
                            return article_list
                        else:
                            self.log_error("ê²Œì‹œê¸€ ëª©ë¡ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
                            self.log_error(f"ì‘ë‹µ êµ¬ì¡°: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
                            if 'result' in data:
                                self.log_error(f"result êµ¬ì¡°: {list(data['result'].keys()) if isinstance(data['result'], dict) else 'Not a dict'}")
                            return []
                    else:
                        response_text = await response.text()
                        self.log_error(f"ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: HTTP {response.status}")
                        self.log_error(f"ì‘ë‹µ ë‚´ìš©: {response_text}")
                        return []
                        
        except Exception as e:
            self.log_error(f"ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            self.log_error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []
    
    async def get_article_content(self, cafe_id: str, article_id: str, retry_count: int = 0) -> Optional[tuple[str, datetime]]:
        """ê²Œì‹œê¸€ ë‚´ìš© ì¡°íšŒ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        try:
            self.log_info(f"ê²Œì‹œê¸€ ë‚´ìš© ì¡°íšŒ ì‹œì‘ (ì¹´í˜ ID: {cafe_id}, ê²Œì‹œê¸€ ID: {article_id}, ì¬ì‹œë„: {retry_count})")
            
            # ì˜¬ë°”ë¥¸ ë„¤ì´ë²„ API ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
            url = f"https://article.cafe.naver.com/gw/v3/cafes/{cafe_id}/articles/{article_id}"
            params = {
                "query": "",
                "boardType": "L",  # ê¸°ë³¸ê°’, í•„ìš”ì‹œ ë™ì ìœ¼ë¡œ ì„¤ì •
                "useCafeId": "true",
                "requestFrom": "A"
            }
            
            self.log_info(f"ìš”ì²­ URL: {url}")
            self.log_info(f"ìš”ì²­ íŒŒë¼ë¯¸í„°: {params}")
            self.log_info(f"ìš”ì²­ í—¤ë”: {self.headers}")
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, headers=self.headers) as response:
                    self.log_info(f"ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status}")
                    self.log_info(f"ì‘ë‹µ í—¤ë”: {dict(response.headers)}")
                    
                    if response.status == 200:
                        data = await response.json()
                        self.log_info(f"ì‘ë‹µ ë°ì´í„° í‚¤: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
                        
                        # ì‹œìŠ¤í…œ ì—ëŸ¬ ì²´í¬
                        if 'error_code' in data and data['error_code'] == '000':
                            error_msg = data.get('message', 'Unknown system error')
                            self.log_error(f"ë„¤ì´ë²„ ì‹œìŠ¤í…œ ì—ëŸ¬ ë°œìƒ: {error_msg}")
                            
                            # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ)
                            if retry_count < 3:
                                self.log_info(f"ì‹œìŠ¤í…œ ì—ëŸ¬ë¡œ ì¸í•œ ì¬ì‹œë„ {retry_count + 1}/3")
                                await asyncio.sleep(2 ** retry_count)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                                return await self.get_article_content(cafe_id, article_id, retry_count + 1)
                            else:
                                self.log_error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {article_id}")
                                return None, None
                        
                        # ìƒˆë¡œìš´ API ì‘ë‹µ êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •
                        if 'result' in data and 'article' in data['result'] and 'contentHtml' in data['result']['article']:
                            content = data['result']['article']['contentHtml']
                            self.log_info(f"ê²Œì‹œê¸€ ë‚´ìš© ì¡°íšŒ ì„±ê³µ (ê¸¸ì´: {len(content)}ì)")
                            
                            # ì¶”ê°€ ì •ë³´ ë¡œê¹…
                            article_info = data['result']['article']
                            self.log_info(f"ê²Œì‹œê¸€ ì œëª©: {article_info.get('subject', 'N/A')}")
                            self.log_info(f"ì‘ì„±ì: {article_info.get('writer', {}).get('nick', 'N/A')}")
                            
                            # ìƒì„±ì¼ ì •ë³´ ë¡œê¹… ë° ë³€í™˜
                            write_date = article_info.get('writeDate')
                            created_at = self._convert_write_date(write_date)
                            
                            if created_at:
                                self.log_info(f"ê²Œì‹œê¸€ ìƒì„±ì¼: {created_at}")
                            else:
                                self.log_info("ê²Œì‹œê¸€ ìƒì„±ì¼ ì •ë³´ ì—†ìŒ")
                            
                            # contentì™€ created_atì„ í•¨ê»˜ ë°˜í™˜ (íŠœí”Œ í˜•íƒœ)
                            return content, created_at
                        else:
                            self.log_error(f"ê²Œì‹œê¸€ ë‚´ìš© ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
                            self.log_error(f"ì‘ë‹µ êµ¬ì¡°: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
                            if 'result' in data:
                                self.log_error(f"result êµ¬ì¡°: {list(data['result'].keys()) if isinstance(data['result'], dict) else 'Not a dict'}")
                            if 'result' in data and 'article' in data['result']:
                                self.log_error(f"article êµ¬ì¡°: {list(data['result']['article'].keys()) if isinstance(data['result']['article'], dict) else 'Not a dict'}")
                            return None, None
                    else:
                        response_text = await response.text()
                        self.log_error(f"ê²Œì‹œê¸€ ë‚´ìš© ì¡°íšŒ ì‹¤íŒ¨: HTTP {response.status}")
                        self.log_error(f"ì‘ë‹µ ë‚´ìš©: {response_text}")
                        return None, None
                        
        except Exception as e:
            self.log_error(f"ê²Œì‹œê¸€ ë‚´ìš© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            self.log_error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return None, None
    
    async def get_article_comments(self, cafe_id: str, article_id: str) -> List[Dict[str, Any]]:
        """ê²Œì‹œê¸€ì˜ ëŒ“ê¸€ ëª©ë¡ ì¡°íšŒ"""
        try:
            self.log_info(f"ëŒ“ê¸€ ëª©ë¡ ì¡°íšŒ ì‹œì‘ (ì¹´í˜ ID: {cafe_id}, ê²Œì‹œê¸€ ID: {article_id})")
            
            # ëŒ“ê¸€ ì¡°íšŒ API ì—”ë“œí¬ì¸íŠ¸ (ê²Œì‹œê¸€ ë‚´ìš© ì¡°íšŒ ì‹œ í•¨ê»˜ ë°›ì•„ì˜´)
            # ì‹¤ì œë¡œëŠ” ê²Œì‹œê¸€ ë‚´ìš© ì¡°íšŒ ì‹œ comments ì •ë³´ê°€ í•¨ê»˜ í¬í•¨ë¨
            url = f"https://article.cafe.naver.com/gw/v3/cafes/{cafe_id}/articles/{article_id}"
            params = {
                "query": "",
                "boardType": "L",
                "useCafeId": "true",
                "requestFrom": "A"
            }
            
            self.log_info(f"ìš”ì²­ URL: {url}")
            self.log_info(f"ìš”ì²­ íŒŒë¼ë¯¸í„°: {params}")
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, headers=self.headers) as response:
                    self.log_info(f"ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status}")
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        # ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ
                        if 'result' in data and 'comments' in data['result']:
                            comments_data = data['result']['comments']
                            
                            if 'items' in comments_data:
                                comments = comments_data['items']
                                self.log_info(f"ëŒ“ê¸€ {len(comments)}ê°œ ì¡°íšŒ ì™„ë£Œ")
                                
                                # ëŒ“ê¸€ ë°ì´í„° ì •ë¦¬
                                processed_comments = []
                                for comment in comments:
                                    # updateDateë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                                    created_at = self._convert_write_date(comment.get('updateDate'))
                                    
                                    processed_comment = {
                                        'comment_id': str(comment.get('id', '')),
                                        'ref_id': str(comment.get('refId', '')),
                                        'writer_nickname': comment.get('writer', {}).get('nick', ''),
                                        'writer_id': comment.get('writer', {}).get('id', ''),
                                        'writer_member_key': comment.get('writer', {}).get('memberKey', ''),
                                        'content': comment.get('content', ''),
                                        'created_at': created_at,
                                        'member_level': comment.get('memberLevel', 0),
                                        'is_deleted': comment.get('isDeleted', False),
                                        'is_article_writer': comment.get('isArticleWriter', False),
                                        'is_new': comment.get('isNew', False)
                                    }
                                    processed_comments.append(processed_comment)
                                    
                                    self.log_info(f"ëŒ“ê¸€ {processed_comment['comment_id']} ì²˜ë¦¬ ì™„ë£Œ: {processed_comment['writer_nickname']}")
                                
                                return processed_comments
                            else:
                                self.log_info("ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤")
                                return []
                        else:
                            self.log_info("ëŒ“ê¸€ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
                            return []
                    else:
                        response_text = await response.text()
                        self.log_error(f"ëŒ“ê¸€ ì¡°íšŒ ì‹¤íŒ¨: HTTP {response.status}")
                        self.log_error(f"ì‘ë‹µ ë‚´ìš©: {response_text}")
                        return []
                        
        except Exception as e:
            self.log_error(f"ëŒ“ê¸€ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            self.log_error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []
    
    async def get_articles_with_content(self, cafe_id: str, menu_id: str = "", per_page: int = 20) -> List[NaverCafeArticle]:
        """ê²Œì‹œê¸€ ëª©ë¡ê³¼ ë‚´ìš©ì„ í•¨ê»˜ ì¡°íšŒ (ê°œì„ ëœ ë²„ì „)"""
        try:
            self.log_info(f"ê²Œì‹œê¸€ê³¼ ë‚´ìš© ì¡°íšŒ ì‹œì‘ (ì¹´í˜ ID: {cafe_id}, ë©”ë‰´ ID: {menu_id})")
            
            # ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ
            articles = await self.get_article_list(cafe_id, menu_id, 1, per_page)
            
            if not articles:
                self.log_warning("ìˆ˜ì§‘í•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            # ê° ê²Œì‹œê¸€ì˜ ë‚´ìš© ì¡°íšŒ
            articles_with_content = []
            for i, article in enumerate(articles):
                try:
                    self.log_info(f"ê²Œì‹œê¸€ {i+1}/{len(articles)} ë‚´ìš© ì¡°íšŒ ì¤‘...")
                    
                    content_html, created_at = await self.get_article_content(cafe_id, article.article_id)
                    if content_html:
                        article.content = self.parse_content_html(content_html)
                        self.log_info(f"ê²Œì‹œê¸€ {article.article_id} ë‚´ìš© íŒŒì‹± ì™„ë£Œ")
                        
                        # ìƒì„±ì¼ì´ ì—†ëŠ” ê²½ìš° ë‚´ìš© ì¡°íšŒì—ì„œ ì–»ì€ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                        if not article.created_at and created_at:
                            article.created_at = created_at
                            self.log_info(f"ê²Œì‹œê¸€ {article.article_id} ìƒì„±ì¼ ì—…ë°ì´íŠ¸: {created_at}")
                        elif article.created_at:
                            self.log_info(f"ê²Œì‹œê¸€ {article.article_id} ê¸°ì¡´ ìƒì„±ì¼ ìœ ì§€: {article.created_at}")
                    else:
                        self.log_warning(f"ê²Œì‹œê¸€ {article.article_id} ë‚´ìš© ì¡°íšŒ ì‹¤íŒ¨")
                        article.content = ""
                    
                    # ìƒì„±ì¼ ì •ë³´ ë¡œê¹…
                    if article.created_at:
                        self.log_info(f"ê²Œì‹œê¸€ {article.article_id} ìµœì¢… ìƒì„±ì¼: {article.created_at}")
                    else:
                        self.log_warning(f"ê²Œì‹œê¸€ {article.article_id} ìƒì„±ì¼ ì •ë³´ ì—†ìŒ")
                    
                    articles_with_content.append(article)
                    
                    # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ (ë„¤ì´ë²„ API ì œí•œ ê³ ë ¤)
                    await asyncio.sleep(0.2)
                    
                except Exception as e:
                    self.log_error(f"ê²Œì‹œê¸€ {article.article_id} ë‚´ìš© ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
                    article.content = ""
                    articles_with_content.append(article)
                    continue
            
            self.log_info(f"ê²Œì‹œê¸€ê³¼ ë‚´ìš© ì¡°íšŒ ì™„ë£Œ: {len(articles_with_content)}ê°œ")
            return articles_with_content
            
        except Exception as e:
            self.log_error(f"ê²Œì‹œê¸€ê³¼ ë‚´ìš© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    async def get_board_title_and_content(self, cafe_id: str, menu_id: str = "", per_page: int = 20) -> str:
        """ê²Œì‹œíŒì˜ ê²Œì‹œê¸€ ì œëª©ê³¼ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (ì°¸ê³  ì½”ë“œ ê¸°ë°˜)"""
        try:
            self.log_info(f"ê²Œì‹œê¸€ ì œëª©ê³¼ ë‚´ìš© ì¡°íšŒ ì‹œì‘ (ì¹´í˜ ID: {cafe_id}, ë©”ë‰´ ID: {menu_id})")
            
            # ê²Œì‹œê¸€ ëª©ë¡ ì¡°íšŒ
            articles = await self.get_article_list(cafe_id, menu_id, 1, per_page)
            
            if not articles:
                return "ìˆ˜ì§‘í•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤."
            
            result = ""
            content_cnt = 1
            
            for article in articles:
                try:
                    self.log_info(f"ê²Œì‹œê¸€ {content_cnt}/{len(articles)} ì²˜ë¦¬ ì¤‘...")
                    
                    # ê²Œì‹œê¸€ ë‚´ìš© ì¡°íšŒ
                    board_content, created_at = await self.get_article_content(cafe_id, article.article_id)
                    if board_content:
                        content = self.parse_content_html(board_content)
                        
                        # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½
                        if content and len(content) > 200:
                            content = content[:200] + "..."
                        
                        result += f"ê²Œì‹œê¸€ {content_cnt}:\nì œëª©: {article.subject}\nì‘ì„±ì: {article.writer_nickname}\në‚´ìš©: {content}\n\n"
                    else:
                        result += f"ê²Œì‹œê¸€ {content_cnt}:\nì œëª©: {article.subject}\nì‘ì„±ì: {article.writer_nickname}\në‚´ìš©: ì¡°íšŒ ì‹¤íŒ¨\n\n"
                    
                    content_cnt += 1
                    
                    # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
                    await asyncio.sleep(0.3)
                    
                except Exception as e:
                    self.log_error(f"ê²Œì‹œê¸€ {content_cnt} ë‚´ìš© ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
                    result += f"ê²Œì‹œê¸€ {content_cnt}:\nì œëª©: {article.subject}\në‚´ìš©: ì˜¤ë¥˜ ë°œìƒ\n\n"
                    content_cnt += 1
                    continue
            
            self.log_info(f"ê²Œì‹œê¸€ ì œëª©ê³¼ ë‚´ìš© ì¡°íšŒ ì™„ë£Œ: {content_cnt-1}ê°œ")
            return result
            
        except Exception as e:
            self.log_error(f"ê²Œì‹œê¸€ ì œëª©ê³¼ ë‚´ìš© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return f"ê²Œì‹œê¸€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
    
    async def get_articles_with_content_and_comments(self, cafe_id: str, menu_id: str = "", per_page: int = 20, target_date: Optional[str] = None, limit: int = 0) -> List[Dict[str, Any]]:
        """
        ê²Œì‹œê¸€ ëª©ë¡, ë‚´ìš©, ëŒ“ê¸€ì„ í•¨ê»˜ ì¡°íšŒ
        
        Args:
            cafe_id: ì¹´í˜ ID
            menu_id: ë©”ë‰´ ID (ê¸°ë³¸ê°’: "")
            per_page: í˜ì´ì§€ë‹¹ ê²Œì‹œê¸€ ìˆ˜ (ê¸°ë³¸ê°’: 20)
            target_date: ëŒ€ìƒ ë‚ ì§œ (ê¸°ë³¸ê°’: None)
            limit: ìˆ˜ì§‘í•  ìµœëŒ€ ê°œìˆ˜ (0ì´ë©´ ë¬´ì œí•œ)
        """
        try:
            self.log_info(f"ê²Œì‹œê¸€ê³¼ ë‚´ìš©, ëŒ“ê¸€ ì¡°íšŒ ì‹œì‘ (ì¹´í˜ ID: {cafe_id}, ë©”ë‰´ ID: {menu_id}, ë‚ ì§œ: {target_date})")
            
            # limitì´ ì„¤ì •ëœ ê²½ìš° í˜ì´ì§€ë‹¹ ê°œìˆ˜ë¥¼ limitì— ë§ì¶° ì¡°ì •
            if limit > 0:
                initial_per_page = min(per_page * 4, limit) if target_date else min(per_page, limit)
            else:
                initial_per_page = per_page * 4 if target_date else per_page
            
            articles = await self.get_article_list(cafe_id, menu_id, 1, initial_per_page)
            
            if not articles:
                self.log_warning("ìˆ˜ì§‘í•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            self.log_info(f"ì´ˆê¸° ê²Œì‹œê¸€ {len(articles)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            
            # ê° ê²Œì‹œê¸€ì˜ ë‚´ìš©ê³¼ ëŒ“ê¸€ ì¡°íšŒ (ìƒì„±ì¼ ì •ë³´ í¬í•¨)
            articles_with_content_and_comments = []
            consecutive_404_errors = 0
            max_404_errors = 5
            
            for i, article in enumerate(articles):
                try:
                    # limit ì²´í¬ (0ì´ë©´ ë¬´ì œí•œ)
                    if limit > 0 and len(articles_with_content_and_comments) >= limit:
                        self.log_info(f"ğŸ“Š ìˆ˜ì§‘ ê°œìˆ˜ ì œí•œ ë„ë‹¬: {limit}ê°œ")
                        break
                    
                    self.log_info(f"ê²Œì‹œê¸€ {i+1}/{len(articles)} ì²˜ë¦¬ ì¤‘... (ID: {article.article_id})")
                    
                    # ê²Œì‹œê¸€ë³„ 5ì´ˆ ë”œë ˆì´ (ê³¼ë¶€í•˜ ë°©ì§€)
                    await asyncio.sleep(5)
                    
                    # ê²Œì‹œê¸€ ë‚´ìš© ì¡°íšŒ
                    content_html, created_at = await self.get_article_content(cafe_id, article.article_id)
                    if content_html:
                        article.content = self.parse_content_html(content_html)
                        self.log_info(f"ê²Œì‹œê¸€ {article.article_id} ë‚´ìš© íŒŒì‹± ì™„ë£Œ")
                        
                        # ìƒì„±ì¼ì´ ì—†ëŠ” ê²½ìš° ë‚´ìš© ì¡°íšŒì—ì„œ ì–»ì€ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                        if not article.created_at and created_at:
                            article.created_at = created_at
                            self.log_info(f"ê²Œì‹œê¸€ {article.article_id} ìƒì„±ì¼ ì—…ë°ì´íŠ¸: {created_at}")
                    else:
                        self.log_warning(f"ê²Œì‹œê¸€ {article.article_id} ë‚´ìš© ì¡°íšŒ ì‹¤íŒ¨")
                        article.content = ""
                    
                    # ëŒ“ê¸€ ì¡°íšŒ
                    comments = await self.get_article_comments(cafe_id, article.article_id)
                    self.log_info(f"ê²Œì‹œê¸€ {article.article_id} ëŒ“ê¸€ {len(comments)}ê°œ ì¡°íšŒ ì™„ë£Œ")
                    
                    # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
                    article_data = {
                        'article': article,
                        'comments': comments,
                        'comment_count': len(comments)
                    }
                    
                    articles_with_content_and_comments.append(article_data)
                    
                    # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
                    await asyncio.sleep(0.3)
                    
                    # limit ì²´í¬ (0ì´ë©´ ë¬´ì œí•œ) - ê²Œì‹œê¸€ ì²˜ë¦¬ í›„ ì²´í¬
                    if limit > 0 and len(articles_with_content_and_comments) >= limit:
                        self.log_info(f"ğŸ“Š ìˆ˜ì§‘ ê°œìˆ˜ ì œí•œ ë„ë‹¬: {limit}ê°œ")
                        break
                    
                except Exception as e:
                    error_msg = str(e)
                    if "404" in error_msg or "Not Found" in error_msg:
                        consecutive_404_errors += 1
                        self.log_error(f"âŒ 404 ì—ëŸ¬ ë°œìƒ (ì—°ì† {consecutive_404_errors}íšŒ): {e}")
                        
                        if consecutive_404_errors >= max_404_errors:
                            self.log_error(f"ğŸš« ì—°ì† 404 ì—ëŸ¬ {max_404_errors}íšŒ ë°œìƒ. 20ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                            await asyncio.sleep(20 * 60)  # 20ë¶„ ëŒ€ê¸°
                            consecutive_404_errors = 0  # ì¹´ìš´í„° ë¦¬ì…‹
                        else:
                            await asyncio.sleep(5)  # 5ì´ˆ ëŒ€ê¸°
                    else:
                        self.log_error(f"ê²Œì‹œê¸€ {article.article_id} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    
                    article_data = {
                        'article': article,
                        'comments': [],
                        'comment_count': 0
                    }
                    articles_with_content_and_comments.append(article_data)
                    continue
            
            # ë‚ ì§œ í•„í„°ë§ (target_dateê°€ ì§€ì •ëœ ê²½ìš°, ë‚´ìš© ì¡°íšŒ í›„ ìˆ˜í–‰)
            if target_date:
                try:
                    from datetime import datetime
                    target_datetime = datetime.strptime(target_date, "%Y-%m-%d")
                    self.log_info(f"ë‚ ì§œ í•„í„°ë§ ì ìš©: {target_date}")
                    
                    # í•´ë‹¹ ë‚ ì§œì˜ ê²Œì‹œê¸€ë§Œ í•„í„°ë§
                    filtered_articles = []
                    for article_data in articles_with_content_and_comments:
                        article = article_data['article']
                        if article.created_at and article.created_at.date() == target_datetime.date():
                            filtered_articles.append(article_data)
                    
                    articles_with_content_and_comments = filtered_articles
                    self.log_info(f"ë‚ ì§œ í•„í„°ë§ í›„ ê²Œì‹œê¸€ ìˆ˜: {len(articles_with_content_and_comments)}ê°œ")
                    
                    # í•„í„°ë§ëœ ê²Œì‹œê¸€ì´ per_pageë³´ë‹¤ ì ìœ¼ë©´ ë” ë§ì€ ê²Œì‹œê¸€ì„ ì¡°íšŒ
                    if len(articles_with_content_and_comments) < per_page:
                        self.log_info(f"í•„í„°ë§ëœ ê²Œì‹œê¸€ì´ {per_page}ê°œë³´ë‹¤ ì ì–´ì„œ ì¶”ê°€ ì¡°íšŒë¥¼ ì‹œë„í•©ë‹ˆë‹¤")
                        # ì´ë¯¸ ì¶©ë¶„í•œ ê²Œì‹œê¸€ì„ ì¡°íšŒí–ˆìœ¼ë¯€ë¡œ í˜„ì¬ ê²°ê³¼ë¥¼ ë°˜í™˜
                        self.log_warning(f"ìš”ì²­ëœ ë‚ ì§œ({target_date})ì— í•´ë‹¹í•˜ëŠ” ê²Œì‹œê¸€ì´ {len(articles_with_content_and_comments)}ê°œë§Œ ìˆìŠµë‹ˆë‹¤")
                        
                except ValueError as e:
                    self.log_error(f"ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {target_date}, ì˜ˆìƒ í˜•ì‹: YYYY-MM-DD")
                    return []
            
            # per_pageë§Œí¼ë§Œ ì²˜ë¦¬
            articles_with_content_and_comments = articles_with_content_and_comments[:per_page]
            self.log_info(f"ìµœì¢… ì²˜ë¦¬í•  ê²Œì‹œê¸€ ìˆ˜: {len(articles_with_content_and_comments)}ê°œ")
            
            self.log_info(f"ê²Œì‹œê¸€ê³¼ ë‚´ìš©, ëŒ“ê¸€ ì¡°íšŒ ì™„ë£Œ: {len(articles_with_content_and_comments)}ê°œ")
            return articles_with_content_and_comments
            
        except Exception as e:
            self.log_error(f"ê²Œì‹œê¸€ê³¼ ë‚´ìš©, ëŒ“ê¸€ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def parse_content_html(self, html_content: str) -> str:
        """HTML ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ íŒŒì‹± (ìƒˆë¡œìš´ ë„¤ì´ë²„ ì—ë””í„° êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •)"""
        try:
            from bs4 import BeautifulSoup
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ìƒˆë¡œìš´ ë„¤ì´ë²„ ì—ë””í„° êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •
            content_parts = []
            
            # 1. ê¸°ë³¸ í…ìŠ¤íŠ¸ ëª¨ë“ˆ (.se-module-text)
            text_elements = soup.select('.se-module-text .se-text-paragraph span')
            if text_elements:
                for element in text_elements:
                    text = element.get_text(strip=True)
                    if text:
                        content_parts.append(text)
            
            # 2. ì œëª© ëª¨ë“ˆ (.se-module-title)
            title_elements = soup.select('.se-module-title .se-text-paragraph span')
            if title_elements:
                for element in title_elements:
                    text = element.get_text(strip=True)
                    if text:
                        content_parts.append(f"ì œëª©: {text}")
            
            # 3. ì´ë¯¸ì§€ ì„¤ëª… (.se-module-image .se-caption)
            image_captions = soup.select('.se-module-image .se-caption')
            if image_captions:
                for caption in image_captions:
                    text = caption.get_text(strip=True)
                    if text:
                        content_parts.append(f"ì´ë¯¸ì§€: {text}")
            
            # 4. ìƒˆë¡œìš´ ë„¤ì´ë²„ ì—ë””í„° êµ¬ì¡° ì§€ì›
            # .se-component-content ë‚´ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ìš”ì†Œ
            component_texts = soup.select('.se-component-content .se-text-paragraph span')
            for element in component_texts:
                text = element.get_text(strip=True)
                if text and text not in content_parts:  # ì¤‘ë³µ ì œê±°
                    content_parts.append(text)
            
            # 5. ê¸°íƒ€ í…ìŠ¤íŠ¸ ìš”ì†Œë“¤ (fallback)
            other_texts = soup.select('p, div, span')
            for element in other_texts:
                text = element.get_text(strip=True)
                if text and len(text) > 5 and text not in content_parts:  # ì¤‘ë³µ ì œê±° ë° ìµœì†Œ ê¸¸ì´ ì²´í¬
                    content_parts.append(text)
            
            # 6. ì „ì²´ í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° fallback
            if not content_parts:
                content = soup.get_text(strip=True)
                # HTML íƒœê·¸ ì œê±° ë° ì •ë¦¬
                import re
                content = re.sub(r'<[^>]+>', '', content)
                content = re.sub(r'\s+', ' ', content)
                content_parts.append(content)
            
            # ë‚´ìš© ê²°í•©
            final_content = ' '.join(content_parts)
            
            # ë‚´ìš© ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ ê²½ìš°)
            if len(final_content) > 2000:
                final_content = final_content[:2000] + "..."
            
            return final_content
            
        except ImportError:
            self.log_error("BeautifulSoupì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install beautifulsoup4")
            return html_content
        except Exception as e:
            self.log_error(f"HTML íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return html_content
    

    
    def _convert_write_date(self, write_date) -> Optional[datetime]:
        """writeDateë¥¼ datetimeìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        if not write_date:
            return None
        
        try:
            # Unix timestamp (ë°€ë¦¬ì´ˆ)ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜ í›„ datetimeìœ¼ë¡œ ë³€í™˜
            timestamp_seconds = int(write_date) / 1000
            created_at = datetime.fromtimestamp(timestamp_seconds)
            # ë¡œê·¸ë¥¼ ì¤„ì´ê¸° ìœ„í•´ debug ë ˆë²¨ë¡œ ë³€ê²½ (ë„ˆë¬´ ë§ì€ ë¡œê·¸ ë°©ì§€)
            # self.log_info(f"ìƒì„±ì¼ ë³€í™˜ ì„±ê³µ: {write_date} -> {created_at}")
            return created_at
        except (ValueError, TypeError) as e:
            self.log_error(f"ìƒì„±ì¼ ë³€í™˜ ì‹¤íŒ¨: {write_date}, ì˜¤ë¥˜: {str(e)}")
            return None
    
    def get_cafe_name_by_id(self, cafe_id: str) -> Optional[str]:
        """ì¹´í˜ IDë¡œ ì¹´í˜ ì´ë¦„ ì¡°íšŒ"""
        for name, id_val in self.cafe_ids.items():
            if id_val == cafe_id:
                return name
        return None
    
    def get_cafe_id_by_name(self, cafe_name: str) -> Optional[str]:
        """ì¹´í˜ ì´ë¦„ìœ¼ë¡œ ì¹´í˜ ID ì¡°íšŒ"""
        return self.cafe_ids.get(cafe_name)
    
    def list_cafes(self) -> Dict[str, str]:
        """ì§€ì›í•˜ëŠ” ì¹´í˜ ëª©ë¡ ë°˜í™˜"""
        return self.cafe_ids.copy()
